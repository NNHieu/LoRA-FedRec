MODEL:
  gmf_emb_size: 16
  mlp_emb_size: 64
  mlp_layer_dims: [128, 64, 32, 16]
  dropout: 0.
  use_lora: True
  lora_r: 8
  lora_alpha: 8

EXP:
  output_dir: exp/lora8_ncf16