MODEL:
  gmf_emb_size: 16
  mlp_emb_size: 64
  mlp_layer_dims: [128, 64, 32, 16]
  dropout: 0.
  use_lora: True
  lora_r: 2
  lora_alpha: 2
  freeze_B: True

EXP:
  output_dir: exp/lora2_ncf16_freezeB