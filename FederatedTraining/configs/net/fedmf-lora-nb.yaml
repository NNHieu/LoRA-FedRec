init:
  _target_: fedlib.lora.models.FedLoraMF
  gmf_emb_size: 64
  lora_rank: 2
  lora_alpha: ${net.init.lora_rank}
  freeze_B: True
normalize_B: True
name: fedmf${net.init.gmf_emb_size}_lora${net.init.lora_rank}_nb